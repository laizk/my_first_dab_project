# This workflow validates, deploys, and runs the specified bundle
# within a pre-production target named "prod".
name: 'Prod deployment'

# Ensure that only a single job or workflow using the same concurrency group
# runs at a time.
concurrency:
  group: prod-deployment
  cancel-in-progress: true

# Trigger this workflow whenever a pull request is opened against the repo's
# main branch or an existing pull request's head branch is updated.
on:
  push:
    branches:
      - master

jobs:
  test_package:
    name: "Checkout, Install, and Test Package"
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install package in editable mode
        run: |
          python -m pip install --upgrade pip
          pip install -e .

      - name: Install dependencies
        run: |
          # Install the dependencies defined in the package.
          # This will automatically install any dependencies listed in setup.py.
          pip install -r requirements-dev.txt

      - name: Run tests
        env:
           DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST_PROD }}
           DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN_PROD }} 
           DATABRICKS_SERVERLESS_COMPUTE_ID: auto    
        run: |
          # Run the tests defined in the package.
          # This will automatically discover and run tests in the "tests" directory.
          python -m pytest


  # Used by the "pipeline_update" job to deploy the bundle.
  # Bundle validation is automatically performed as part of this deployment.
  # If validation fails, this workflow fails.
  deploy:
    name: 'Deploy bundle'
    runs-on: ubuntu-latest
    needs:
      - test_package  # Ensure the package is tested before deployment.

    steps:
      # Check out this repo, so that this workflow can access it.
      - name: Checkout repository
        uses: actions/checkout@v3

      # Download the Databricks CLI.
      # See https://github.com/databricks/setup-cli
      - name: Setup Databricks CLI
        uses: databricks/setup-cli@main

      # Deploy the bundle to the "prod" target as defined
      # in the bundle's settings file.
      - name: Deploy bundle
        # The "databricks bundle deploy" command will automatically validate the bundle.
        # If validation fails, this workflow will fail.
        run: databricks bundle deploy
        working-directory: .
        env:
          DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN }}
          DATABRICKS_BUNDLE_ENV: prod

  # Validate, deploy, and then run the bundle.
  pipeline_update:
    name: 'Run pipeline update'
    runs-on: ubuntu-latest

    # Run the "deploy" job first.
    needs:
      - deploy

    steps:
      # Check out this repo, so that this workflow can access it.
      - uses: actions/checkout@v3

      # Use the downloaded Databricks CLI.
      - uses: databricks/setup-cli@main

      # Run the Databricks workflow named "my-job" as defined in the
      # bundle that was just deployed.
      - run: databricks bundle run demo_project_01_pipeline --refresh-all
        working-directory: .
        env:
          DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN_PROD }}
          DATABRICKS_BUNDLE_ENV: prod

      - run: databricks bundle run demo_project_01_job
        working-directory: .
        env:
          DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN_PROD }}
          DATABRICKS_BUNDLE_ENV: prod          